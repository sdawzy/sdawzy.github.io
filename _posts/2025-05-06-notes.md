---
layout: archive
title: "Study notes: Well-behaved training in Deep Neural Networks"
date: 2025-05-06
permalink: /posts/2025/05/helvetica/en
tags:
  - deep learning
  - machine learning
language: en
---

Study notes for [The lazy (NTK) and rich (ÂµP) regimes: A gentle tutorial](https://arxiv.org/abs/2404.19719) by Dhruva Karkada and [A Spectral Condition for Feature Learning](https://arxiv.org/abs/2310.17813) by Greg Yang, James B. Simon and Jeremy Bernstein. 

Core Idea
======
For a deep neural network, it is crucial to ensure *well-behaved training*, where updates in response to the training data are **relevant, efficient, and balanced across neurons and layers**. One effective approach to achieving this is by carefully designing the **initialization strategy** for each layer and assigning **layer-specific learning rates**. In this framework, the training dynamics are controlled by a single degree of freedom, known as **richness**, which governs the extent of feature learning versus kernel-like (lazy) behavior.


Mathematical Notation
======
The **scaling notation** $$O, \Omega$$ and $$\Theta$$ will be commonly used in this note. Given functions $$f$$ and $$g$$. We say $$f=O(g)$$ if $$f$$ *scales no faster than* $$g$$, i.e., there exists a constant $$C>0$$ such that $$f(n)\leq C\cdot g(n)$$ for sufficiently large $$n$$; we say $$f=\Omega(g)$$ if $$f$$ *scales at least as fast as* $$g$$, i.e., there exists a constant $$c>0$$ such that $$f(n)\geq c\cdot g(n)$$ for sufficiently large $$n$$; we say $$f=\Theta(g)$$ if $$f=O(g)$$ and $$f=\Omega(g)$$. 

Regarding vector and matrix operations, I use $$\mathbf{a} \otimes \mathbf{b}$$ to denote outer-product of two vectors $$\mathbf{a}$$ and $$\mathbf{b}$$. $$\Delta$$ is used to denote changes, especially changes across a single optimization step. 

Regarding deep learning, I primarily focus on multilayer perceptron (MLP). In this note, I use $$W_l$$ to denote the weight matrix of the $$l$$th layer, $$g_l$$ to denote the scalar *gradient multiplier* at layer $$l$$, and $$h_l(\mathbf{x})\in \mathbb{R}^{n_l}$$ to denote the features of input $$\mathbf{x}$$ at layer $$l$$ with size $$n_l$$. Formally, $$h_l(\mathbf{x})$$ is recursively defined as 

$$
h_l(\mathbf{x})=\left\{
\begin{array}{ll}
g_lW_lh_{l-1}(\mathbf{x}) & \text{if } l > 0 \\
\mathbf{x} & \text{if } l = 0
\end{array}
\right.
$$

In addition, $$\mathcal{L}$$ is used to denote the scalar loss. 


Desideratum
======
What are the criteria to say a training as *well-behaved*? Let's first consider how a signal from a single input $$\mathbb{x}$$ changes after a gradient descend step. At layer $$l, l>0$$, after updating the weights by $$\Delta W_l$$, the new representation becomes 

$$
h_l+\Delta h_l = g_l(W_l+\Delta W_l)(h_{l-1}+\Delta h_{l_1})
$$

By substituting the expression for $$h_l$$, we obtain an expression for the representation update as the sum of three terms: 

$$
\Delta h_l = \underbrace{g_l\Delta W_l h_{l-1}}[layer] + \underbrace{g_lW_l\Delta h_{l_1}}[passthrough] + \underbrace{g_l\Delta W_l \Delta h_{l-1}}[interaction]
$$

The first term *layer* is induced by the update of current layer's weight $$\Delta W_l$$; the second term *passthrough* is induced by the update of previous layer $$\Delta h_{l_1}$$ passing through the old weight $$W_l$$; the third term *interaction* captures the interaction between the layer update and the previous representation update. 

For a well-behaved training, we expect that **each representation update $$h_l$$ is controlled**. Specifically, a well-behaved training must satisfy the following three criteria: 

- **Useful Update Criterion (UUC)**: each representation update should contribute to optimizing the loss. The scale of that contribution should be the same for each representation update. Mathematically speaking, for $$l\geq 1$$,

$$
\left| \frac{\partial \mathcal{L}}{\partial h_l}^T \Delta h_l \right| = \Theta(1)
$$

- **Maximality Criterion (MAX)**: A layer's weight update $$\Delta W_l$$ should contribute non-negligibly to the following representation update $$\Delta h_l$$. Any layer's weight should not be frozen in training. Mathematically speaking, for $$l\geq 1$$,

$$
\left\Vert g_l \Delta W_l h_{l-1} \right\Vert = \Theta(\Delta h_l)
$$

- **Nontriviality Criterion(NTC)**: the updates to the outputs shouldn't scale with the width of hidden layers. This ensures that the loss decreases at a width-independent rate. Mathematically speaking, for the last layer $$k$$, 

$$
\left\Vert \Delta h_k \right\Vert = \Theta(1)
$$


Assumptions
======

- *Model Simplicity*: we temporarily ignore activation functions between each layer. Moreover, we particularly focus on **3-layer** MLP, $$h_3(\mathbf{x}):= g_3W_3g_2W_2g_1W_1\mathbf{x}=g_3W_3h_2(\mathbf{x})$$. 
- *Wide Hidden Layers*: we assume both of the two hidden layers have the same scale $$n$$ that is far larger than either the input or output size: 

$$ 
n = \Theta(n_1) = \Theta(n_2) >> 1 = \Theta(n_3) = \Theta(n_0)
$$

- *Gaussian Initialization*: we assume each layer's weight matrix $$W_l$$ is initialized with Gaussian initialization with scale $$\sigma_l$$, i.e., every entry of $$W_l$$ is drawn from $$\mathcal{N}(0, \sigma_l^2)$$. 

- *Bounded Input*: we assume the input $$\mathbf{x}$$ is bounded, and each entry of $$\mathbf{x}$$ is independent of $$n_0$$, i.e., $$\Vert h_0(\mathbf{x})\Vert =\Vert \mathbf{x}\Vert=\Theta(n_0)$$. 

Richness Scale Derivation
=======



