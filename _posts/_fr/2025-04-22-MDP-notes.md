---
layout: archive-fr
title: 'Study notes--Intro to MDP'
date: 2099-04-22
permalink: /posts/2025/04/mdp/fr
tags:
  - notes
  - reinforcement learning
  - statistics
language: en
---

Study notes of Intro to MDP by Martin Puterman and Timothy Chan

Chapter 2
======
(Apr 22)

The concept of MDP (Markov Decision Process) does not sound unfamiliar to me. It's the foundation of modern RL (reinforcement learning) algorithms. The first thing to do in dealing with an RL problem is to specify its underlying MDP. 

To fully specify an MDP model, we need
* the **planning horizon** $$N$$: the time interval over which decisions are made. An MDP model is either a **finite horizon model** with a bounded time interval divided into $$N-1$$ periods, or an **infinite horizon model** with an unbounded time divided into an infinite number of periods.
* the **state space** $$\mathcal{S}$$: the set of all **states** $$s$$ that contain all the relevant information available to make a decision. States must be mutually exclusive and exhaustive, i.e., at each decision epoch the system must be in one state of $$\mathcal{S}$$. 
* the **action set** $$A_s$$ for each state $$s\in\mathcal{S}$$: the set of all actions available to the decision maker in a particular state. Actions are mutually exclusive. 
* the **transition probabilities** $$p_n(j\vert s,a)$$: the probability that the system state becomes $$j$$ at decision epoch $$n+1$$ when the decision maker chooses action $$a$$ in state $$s$$. 
* the **rewards** $$r_n(s,a,j)$$ or $$r_n(s,a)$$: $$r_n(s,a,j)$$ denotes the reward received in period $$n$$ when the decision maker chooses action $$a$$ in state $$s$$ and the system transitions to state $$j$$; $$r_n(s,a)$$ denotes reward received in period $$n$$ when the decision maker chooses action $$a$$ in state $$s$$, regardless of the next state. If the optimality criteria of the decision maker is to maximize the *expected* reward, then $$r_n(s,a)$$ represents the *expected* reward in period $$n$$, where the expectation is taken over all the possible states at decision epoch $$n+1$$. 

## Decision Rules
Decision rules can be classified on the basis of these two independent dimensions:
- Information: **Markovian(M)** or **history-dependent(H)**
- Mechanism: **deterministc(D)** or **randomized(R)**

Note: can we say that any history-dependent model is equivalent to some Markovian model, i.e., all the states are defined to be the whole history?

Based those classifications, there are 4 types of decision rules $$D^{MD}, D^{HD}, D^{MR}, D^{HR}$$. 

A **policy $$\pi$$**, a.k.a., contigency plan or strategy, is a sequence of decision rules, one for each decision epoch. The four classes of decision rules defined above form four classes of policies $$\Pi^{MD}, \Pi^{HD}, \Pi^{MR}$$ and $$\Pi^{HR}$$. If the decision rule is the same in every epoch, we call the policy **stationary**. 

Given the distribution of the starting state and the state transition dynamics (transition probabilities), once a policy is chosen, the probabilistic evolution of a Markov decision process is completely determined. Therefore, in a finite horizon model, a policy and initial state distribution generates the stochastic process 

$$(X_1, Y_1, X_2, Y_2, ..., X_{N-1}, Y_{N-1}, X_N)$$

and in an infinite horizon model,

$$(X_1, Y_1, X_2, Y_2, ...)$$

where $$Y_i$$ is the action taken at decision epoch $$i$$. 

Those processes generate corresponding stochastic process of rewards: 

$$(r_1(X_1, Y_1, X_2),  r_2(X_2, Y_2, X_3), ...,r_{N-1}(X_{N-1}, Y_{N-1}, X_N), r_N(X_N))=(R_1,R_2,...,R_N)$$







